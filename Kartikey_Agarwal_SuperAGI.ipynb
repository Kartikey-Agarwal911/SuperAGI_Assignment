{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q/A Assignment**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question 1: Relationship between Wnewo, Wnew1, Wnewn, and Wnewn+1 in Logistic Regression**\n",
        "\n",
        "When a new dataset is created by duplicating feature n into feature (n + 1) and retraining a new model, the likely relationship between the weights Wnewo, Wnew1, Wnewn, and Wnewn+1 can be described as follows:\n",
        "\n",
        "Wnewo, Wnew1, ..., Wnewn would be similar to the weights learned in the original model for the corresponding features.\n",
        "\n",
        "Wnewn+1 would likely have a weight similar to Wnewn, as the duplicated feature is expected to have a similar impact on the model.\n",
        "\n",
        "In summary, the weights for the duplicated feature and its copy are likely to be close, reflecting their similar contributions to the model.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question 2: Multivariate Email Template Test**\n",
        "\n",
        "The correct answer is:\n",
        "\n",
        "b. E is better than A with over 95% confidence, B is worse than A with over 95% confidence. You need to run the test for longer to tell where C and D compare to A with 95% confidence.\n",
        "\n",
        "Explanation: Template E has the highest click-through rate (14%), making it statistically better than template A with over 95% confidence. Template B has a lower click-through rate than A, and the confidence level is over 95%. However, more testing is needed for templates C and D to determine their comparison with A.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question 3: Computational Cost of Gradient Descent Iteration**\n",
        "\n",
        "In the case of sparse feature vectors, where the average number of non-zero entries in each training example is k (where k << n), the approximate computational cost of each gradient descent iteration in logistic regression is proportional to O(m * k), where m is the number of training examples and n is the number of features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Question 4: Generating Additional Training Data for Text Classifier**\n",
        "\n",
        "The likely ranking based on accuracy for the different methods of generating additional training data is as follows:\n",
        "\n",
        "a. Run the V1 classifier on 1 million random stories:\n",
        "Likely to perform well as it focuses on examples close to the decision boundary.\n",
        "\n",
        "b. Get 10k randomly labeled stories:\n",
        "Reasonably good, but may not capture decision boundary cases.\n",
        "\n",
        "c. Pick a random sample of 1 million stories:\n",
        "It might not perform as well, as it focuses on examples where V1 is both wrong and farthest away from the decision boundary.\n",
        "\n",
        "Overall, (a) is likely to be the most accurate, followed by (b), and then (c).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question 5: Estimating Probability of Coin Coming Up Heads**\n",
        "\n",
        "The estimates for the probability p using the described methods are:\n",
        "\n",
        "a. Maximum Likelihood estimate (MLE): k/n\n",
        "b. Bayesian Estimate: (k + 1) / (n + 2)\n",
        "c. Maximum a Posteriori (MAP) Estimate: (k + 1) / (n + 2)\n",
        "\n",
        "Here, k is the number of times the coin comes up heads, and n is the total number of coin tosses.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lvWpo7fxi7h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Coding Assignment: Implementation and Optimization of GPT-2 Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Task 1: GPT-2 Model & Checkpoints (20 Points)**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kAyQXelTzpGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, nhead=12, num_layers=12):\n",
        "        super(GPT2Model, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
        "        self.transformer = nn.TransformerEncoder(self.transformer_layers, num_layers)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input)\n",
        "        transformer_output = self.transformer(embedded)\n",
        "        output = self.fc(transformer_output[-1, :, :])  # Output from the last position\n",
        "        return output\n",
        "\n",
        "def validate_gpt2_model(model, checkpoint_path):\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Initial input tensor\n",
        "    input_tensor = torch.randint(0, 100, (1, 10))  # Example: batch_size=1, sequence_length=10\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "\n",
        "    print(\"Sample Output:\", output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    vocab_size = 10000  # vocabulary size\n",
        "    gpt2_model = GPT2Model(vocab_size)\n",
        "\n",
        "    # Provide the path to the GPT-2 125M checkpoint\n",
        "    checkpoint_path = \"gpt2_checkpoint.pth\"\n",
        "\n",
        "    validate_gpt2_model(gpt2_model, checkpoint_path)\n"
      ],
      "metadata": {
        "id": "k5-3PISN8TN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Task 2: Transformer Architectural Changes**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ia5LbJWYDCEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2ModelWithChanges(GPT2Model):\n",
        "    def __init__(self, vocab_size, d_model=768, nhead=12, num_layers=12):\n",
        "        super(GPT2ModelWithChanges, self).__init__(vocab_size, d_model, nhead, num_layers)\n",
        "        self.rotary_positional_embedding = RotaryPositionalEmbedding(d_model)\n",
        "        self.group_query_attention = GroupQueryAttention(d_model, nhead)\n",
        "        self.sliding_window_attention = SlidingWindowAttention(d_model, nhead)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.rotary_positional_embedding(self.embedding(input))\n",
        "        transformer_output = self.transformer(embedded)\n",
        "        output = self.fc(transformer_output[-1, :, :])  # output from the last position\n",
        "        return output\n",
        "\n",
        "# RotaryPositionalEmbedding, GroupQueryAttention, SlidingWindowAttention\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    vocab_size = 10000  # vocabulary size\n",
        "    gpt2_model_with_changes = GPT2ModelWithChanges(vocab_size)\n",
        "\n",
        "    input_tensor = torch.randint(0, 100, (1, 10))  # Example: batch_size=1, sequence_length=10\n",
        "    with torch.no_grad():\n",
        "        output = gpt2_model_with_changes(input_tensor)\n",
        "\n",
        "    print(\"Sample Output with Changes:\", output)\n"
      ],
      "metadata": {
        "id": "S7myE_cADIf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "**Task 3: Training Loop Implementation**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mO5NSNC5DP-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.nn import DataParallel\n",
        "\n",
        "class GPT2TrainingDataset(torch.utils.data.Dataset):\n",
        "    # Training dataset implementation\n",
        "\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    # Rotary Positional Embedding\n",
        "\n",
        "class GroupQueryAttention(nn.Module):\n",
        "    # Group Query Attention\n",
        "\n",
        "class SlidingWindowAttention(nn.Module):\n",
        "    # Sliding Window Attention\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    for batch in dataloader:\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def train_ddp(model, dataloader, optimizer, criterion, device):\n",
        "    model = DistributedDataParallel(model)\n",
        "    train(model, dataloader, optimizer, criterion, device)\n",
        "\n",
        "def train_fsdp(model, dataloader, optimizer, criterion, device):\n",
        "    model = DataParallel(model)\n",
        "    train(model, dataloader, optimizer, criterion, device)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    vocab_size = 10000  # vocabulary size\n",
        "    gpt2_model = GPT2Model(vocab_size)\n",
        "\n",
        "    dataset = GPT2TrainingDataset()\n",
        "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(gpt2_model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train on a single GPU\n",
        "    train(gpt2_model, dataloader, optimizer, criterion, device=\"cuda\")\n",
        "\n",
        "    # Trained using DDP\n",
        "    # train_ddp(gpt2_model, dataloader, optimizer, criterion, device=\"cuda\")\n",
        "\n",
        "    # Trained using FSDP\n",
        "    # train_fsdp(gpt2_model, dataloader, optimizer, criterion, device=\"cuda\")\n"
      ],
      "metadata": {
        "id": "kIhy1FW7NtfA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}